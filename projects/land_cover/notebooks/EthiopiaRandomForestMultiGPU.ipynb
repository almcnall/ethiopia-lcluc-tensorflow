{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethiopia Random Forest Model\n",
    "\n",
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import rasterio.features as riofeat\n",
    "from osgeo import gdal, osr\n",
    "\n",
    "from cuml.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, \\\n",
    "    precision_score, recall_score, f1_score\n",
    "\n",
    "import cupy as cp\n",
    "import cudf as cf\n",
    "import cuml as cl\n",
    "\n",
    "from cupyx.scipy.ndimage import median_filter\n",
    "from cuml.metrics import accuracy_score\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask_cudf\n",
    "\n",
    "from cuml.dask.ensemble import RandomForestClassifier as cumlDaskRF\n",
    "from sklearn.ensemble import RandomForestClassifier as sklRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(threads_per_worker=1)\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_streams = 8 # Performance optimization\n",
    "print(n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters\n",
    "\n",
    "In addition to the number of examples, random forest fitting performance depends heavily on the number of columns in a dataset and (especially) on the maximum depth to which trees are allowed to grow. Lower max_depth values can greatly speed up fitting, though going too low may reduce accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_step = ['train', 'predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "train_csv = '/att/pubrepo/ILAB/projects/Ethiopia/ethiopia-lcluc/data/random_forest/train_data.csv'\n",
    "seed = 24\n",
    "train_size = 0.80\n",
    "max_feat = 'log2'\n",
    "\n",
    "# Random Forest building parameters\n",
    "n_trees = 100\n",
    "max_feat = 'log2'\n",
    "max_depth = 24 # 12 - bad\n",
    "n_bins = 16\n",
    "n_trees = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CB     B     G     Y     R    RE  NIR1  NIR2  CLASS\n",
      "0        1037   870   966  1049  1084  1540  1810  1627      0\n",
      "1         992   722   614   626   638   755   825   749      0\n",
      "2        1050   794   828   807   732  1596  2398  2112      0\n",
      "3        1089  1029  1239  1385  1526  2216  2913  2684      0\n",
      "4        1122   886   907   948   904  1498  1997  1762      0\n",
      "...       ...   ...   ...   ...   ...   ...   ...   ...    ...\n",
      "1746512  1479  1361  1372  1490  1779  2079  2515  2522      5\n",
      "1746513  1384  1193  1195  1262  1460  1946  2336  2248      5\n",
      "1746514  1609  1631  1699  1945  2307  2667  3166  3025      5\n",
      "1746515  1649  1611  1644  1763  2009  2244  2491  2340      5\n",
      "1746516  1568  1385  1400  1604  1766  1955  2084  1908      5\n",
      "\n",
      "[1746517 rows x 9 columns] <class 'dask_cudf.core.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(train_csv), f'{train_csv} not found.'\n",
    "data_df = dask_cudf.read_csv(train_csv, sep=',')\n",
    "assert not data_df.isnull().values.any(), f'Na found: {train_csv}'\n",
    "print(data_df.compute(), type(data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.sample(frac=1).reset_index(drop=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask_cudf does not support iloc operations, the objects gett converted to plain cudf\n",
    "x = data_df.iloc[:, :-1].astype(np.float32)\n",
    "y = data_df.iloc[:, -1].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 1397213 elements\n",
      "X_test:  349304 elements\n",
      "y_train: 1397213 elements\n",
      "y_test:  349304 elements\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=train_size)\n",
    "del data_df, x, y\n",
    "print(f'X_train: {X_train.shape[0]} elements')\n",
    "print(f'X_test:  {X_test.shape[0]} elements')\n",
    "print(f'y_train: {y_train.shape[0]} elements')\n",
    "print(f'y_test:  {y_test.shape[0]} elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribute data to worker GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_partitions = n_workers\n",
    "\n",
    "def distribute(X, y):\n",
    "\n",
    "    # Partition with Dask\n",
    "    # In this case, each worker will train on 1/n_partitions fraction of the data\n",
    "    X_dask = dask_cudf.from_cudf(X, npartitions=n_partitions)\n",
    "    y_dask = dask_cudf.from_cudf(y, npartitions=n_partitions)\n",
    "\n",
    "    # Persist to cache the data in active memory\n",
    "    X_dask, y_dask = \\\n",
    "      dask_utils.persist_across_workers(c, [X_dask, y_dask], workers=workers)\n",
    "    \n",
    "    return X_dask, y_dask\n",
    "\n",
    "X_train_dask, y_train_dask = distribute(X_train, y_train)\n",
    "X_test_dask, y_test_dask = distribute(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the distributed cuML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.49 s, sys: 2.02 s, total: 6.51 s\n",
      "Wall time: 1min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future: finished, type: cuml.ensemble.randomforestclassifier.RandomForestClassifier, key: _construct_rf-78ea3495-b750-42b1-97b4-e8153f5b3fb5>, <Future: finished, type: cuml.ensemble.randomforestclassifier.RandomForestClassifier, key: _construct_rf-10dc7cad-abad-4692-965c-3f16af863a93>, <Future: finished, type: cuml.ensemble.randomforestclassifier.RandomForestClassifier, key: _construct_rf-bf153b50-3587-4076-bb15-bcf5a5f66003>, <Future: finished, type: cuml.ensemble.randomforestclassifier.RandomForestClassifier, key: _construct_rf-68c62995-9682-454c-9233-44db633384b5>}, not_done=set())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cuml_model = cumlDaskRF(max_depth=max_depth, n_estimators=n_trees, n_bins=n_bins, n_streams=n_streams)\n",
    "cuml_model.fit(X_train_dask, y_train_dask)\n",
    "wait(cuml_model.rfs) # Allow asynchronous training tasks to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Validate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuml_y_pred = cuml_model.predict(X_test_dask).compute().to_array()\n",
    "\n",
    "# Due to randomness in the algorithm, you may see slight variation in accuracies\n",
    "print(\"CuML accuracy:     \", accuracy_score(y_test, cuml_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cuml.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3. Instantiate RandomForest object - FIX this area\n",
    "        # ------------------------------------------------------------------\n",
    "        if args.has_gpu:  # run using RAPIDS library\n",
    "\n",
    "            # initialize cudf data and log into GPU memory\n",
    "            logging.info('Training model via RAPIDS.')\n",
    "\n",
    "            # single gpu setup\n",
    "            x_train = cf.DataFrame.from_pandas(x_train)\n",
    "            x_test = cf.DataFrame.from_pandas(x_test)\n",
    "            y_train = cf.Series(y_train.values)\n",
    "            rf_funct = cumlRFC  # RF Classifier\n",
    "\n",
    "            # TODO: multi gpu setup\n",
    "            # https://github.com/rapidsai/cuml/blob/branch-21.12/notebooks/random_forest_mnmg_demo.ipynb\n",
    "            # cluster = LocalCUDACluster(\n",
    "            # threads_per_worker=1, n_workers=n_workers)\n",
    "            # c = Client(cluster)\n",
    "            # workers = c.has_what().keys()\n",
    "            # rf_funct = cumlRFC_mg\n",
    "\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 4. Fit Model\n",
    "        # ------------------------------------------------------------------\n",
    "        # fit model to training data and predict for accuracy score\n",
    "        rf_model.fit(x_train, y_train)\n",
    "\n",
    "        if args.has_gpu:\n",
    "            acc_score = accuracy_score(\n",
    "                y_test, rf_model.predict(x_test).to_array())\n",
    "            p_score = precision_score(\n",
    "                y_test, rf_model.predict(x_test).to_array(), average='macro')\n",
    "            r_score = recall_score(\n",
    "                y_test, rf_model.predict(x_test).to_array(), average='macro')\n",
    "            f_score = f1_score(\n",
    "                y_test, rf_model.predict(x_test).to_array(), average='macro')\n",
    "        else:\n",
    "            acc_score = accuracy_score(y_test, rf_model.predict(x_test))\n",
    "            p_score = precision_score(y_test, rf_model.predict(x_test), average='macro')\n",
    "            r_score = recall_score(y_test, rf_model.predict(x_test), average='macro')\n",
    "            f_score = f1_score(y_test, rf_model.predict(x_test), average='macro')\n",
    "\n",
    "        logging.info(f'Test Accuracy:  {acc_score}')\n",
    "        logging.info(f'Test Precision: {p_score}')\n",
    "        logging.info(f'Test Recall:    {r_score}')\n",
    "        logging.info(f'Test F-Score:   {f_score}')\n",
    "\n",
    "        # make output directory\n",
    "        os.makedirs(\n",
    "            os.path.dirname(os.path.realpath(args.output_pkl)), exist_ok=True)\n",
    "\n",
    "        # export model to file\n",
    "        try:\n",
    "            joblib.dump(rf_model, args.output_pkl)\n",
    "            logging.info(f'Model has been saved as {args.output_pkl}')\n",
    "        except Exception as e:\n",
    "            logging.error(f'ERROR: {e}')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ilab-kernel]",
   "language": "python",
   "name": "conda-env-ilab-kernel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
